#!/usr/bin/env python3
import requests
import json
import time
import os
import logging
import re
import yaml
import random  # –î–æ–±–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏ –±–ª–æ–∫–æ–≤
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Iterator, Optional, Union

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OpenAI_API:
    """API class for OpenAI-compatible endpoints like LM Studio."""
    
    def __init__(self, base_url: str = "http://localhost:1234/v1", 
                 api_key: str = "lm-studio",
                 max_history_length: int = 10,
                 system_message: str = "You are a helpful assistant.",
                 config_path: str = "config.yaml"):
        """
        Initialize OpenAI API connection.
        
        Args:
            base_url: URL for API (default: "http://localhost:1234/v1")
            api_key: API key (default: "lm-studio")
            max_history_length: Maximum number of messages in history
            system_message: System message to use
            config_path: Path to configuration file
        """
        self.base_url = base_url
        self.api_key = api_key
        self.max_history_length = max_history_length
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ config.yaml
        self.config = self.load_config(config_path)
        self.temperature = self.config.get('temperature', 0.2)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.2
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º
        self.message_history = [
            {"role": "system", "content": system_message}
        ]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        self.check_connection()
    
    def load_config(self, config_path: str) -> dict:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ YAML —Ñ–∞–π–ª–∞."""
        config = {'temperature': 0.2}  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    yaml_config = yaml.safe_load(f)
                    if yaml_config and isinstance(yaml_config, dict):
                        config.update(yaml_config)
                logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {config_path}")
            else:
                logger.warning(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        
        return config
        
    def check_connection(self) -> bool:
        """
        Check if connection to API is working.
        
        Returns:
            bool: True if connection works, False otherwise
        """
        try:
            models_url = f"{self.base_url}/models"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            response = requests.get(models_url, headers=headers, timeout=600)
            
            if response.status_code == 200:
                models = response.json().get("data", [])
                logger.info(f"Connected to API. Available models: {len(models)}")
                return True
            else:
                logger.warning(f"API connection check failed: {response.status_code} - {response.text}")
                return False
        except Exception as e:
            logger.error(f"Error checking API connection: {str(e)}")
            return False
    
    def _truncate_messages_to_fit(self, messages: List[Dict], max_tokens: int) -> List[Dict]:
        """
        –û–±—Ä–µ–∑–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è, —á—Ç–æ–±—ã –æ–Ω–∏ –ø–æ–º–µ—Å—Ç–∏–ª–∏—Å—å –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤.
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            
        Returns:
            List[Dict]: –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        chars_per_token = 4
        total_chars = sum(len(msg["content"]) for msg in messages)
        estimated_tokens = total_chars / chars_per_token
        
        # –ï—Å–ª–∏ –≤–º–µ—Å—Ç–∏–º—Å—è –≤ –ª–∏–º–∏—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        if estimated_tokens <= max_tokens:
            return messages
        
        # –í—ã–¥–µ–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        system_msg = None
        last_user_msg = None
        other_msgs = []
        
        for msg in messages:
            if msg["role"] == "system":
                system_msg = msg
            elif msg["role"] == "user" and last_user_msg is None:
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Å–∞–º–æ–µ –ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                last_user_msg = msg
            else:
                other_msgs.append(msg)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª–æ–≤ —É –Ω–∞—Å —É–∂–µ –∑–∞–Ω—è—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω—ã–º–∏ —Å–æ–æ–±—â–µ–Ω–∏—è–º–∏
        critical_chars = 0
        if system_msg:
            critical_chars += len(system_msg["content"])
        if last_user_msg:
            critical_chars += len(last_user_msg["content"])
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–∫–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª–æ–≤ –æ—Å—Ç–∞–ª–æ—Å—å –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        max_chars = int(max_tokens * chars_per_token)
        remaining_chars = max(0, max_chars - critical_chars)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ—Ç –Ω–æ–≤—ã—Ö –∫ —Å—Ç–∞—Ä—ã–º
        other_msgs.reverse()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è, –ø–æ–∫–∞ –≤–ª–µ–∑–∞—é—Ç
        included_msgs = []
        for msg in other_msgs:
            msg_len = len(msg["content"])
            if msg_len <= remaining_chars:
                included_msgs.append(msg)
                remaining_chars -= msg_len
            else:
                # –ï—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ, –º–æ–∂–Ω–æ –æ–±—Ä–µ–∑–∞—Ç—å, –Ω–æ –ª—É—á—à–µ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å
                break
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
        truncated_messages = []
        if system_msg:
            truncated_messages.append(system_msg)
        truncated_messages.extend(reversed(included_msgs))  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫
        if last_user_msg:
            truncated_messages.append(last_user_msg)
        
        logger.info(f"–û–±—Ä–µ–∑–∞–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–π: –±—ã–ª–æ {len(messages)}, —Å—Ç–∞–ª–æ {len(truncated_messages)}")
        return truncated_messages
    
    def _extract_context_limit(self, error_message: str) -> int:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ.
        
        Args:
            error_message: –°–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
            
        Returns:
            int: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–ª–∏ 4000 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        """
        logger.info(f"–ê–Ω–∞–ª–∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ: {error_message[:200]}")
        
        # –¢–∏–ø–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—à–∏–±–∫–∏ LM Studio
        match = re.search(r'model is loaded with context length of only (\d+) tokens', error_message)
        if match:
            limit = int(match.group(1))
            logger.info(f"–ù–∞–π–¥–µ–Ω –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Ñ–æ—Ä–º–∞—Ç 1): {limit}")
            return max(1000, int(limit * 0.9))
        
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        match = re.search(r'context length of only (\d+) tokens', error_message)
        if match:
            limit = int(match.group(1))
            logger.info(f"–ù–∞–π–¥–µ–Ω –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Ñ–æ—Ä–º–∞—Ç 2): {limit}")
            return max(1000, int(limit * 0.9))
        
        # –§–æ—Ä–º–∞—Ç "Trying to keep X tokens"
        match = re.search(r'Trying to keep the first (\d+) tokens', error_message)
        if match:
            current = int(match.group(1))
            
            # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –æ –ª–∏–º–∏—Ç–µ –º–æ–¥–µ–ª–∏
            limit_match = re.search(r'context length of only (\d+) tokens', error_message)
            if limit_match:
                limit = int(limit_match.group(1))
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ: —Ç–µ–∫—É—â–∏–µ —Ç–æ–∫–µ–Ω—ã = {current}, –ª–∏–º–∏—Ç –º–æ–¥–µ–ª–∏ = {limit}")
                return max(1000, int(limit * 0.9))
            else:
                # –ï—Å–ª–∏ –ª–∏–º–∏—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º 70% –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {current}")
                return max(1000, int(current * 0.7))
        
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ª–∏–º–∏—Ç –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º 4000")
        return 4000
    
    def _estimate_tokens(self, messages: List[Dict]) -> int:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö.
        
        Args:
            messages: –°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        
        Returns:
            int: –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        """
        # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ: 1 —Ç–æ–∫–µ–Ω ~= 4 —Å–∏–º–≤–æ–ª–∞
        chars_per_token = 4
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π (—Ä–æ–ª–∏ –∏ —Ç.–¥.)
        base_tokens = 4 * len(messages)
        
        # –°—á–∏—Ç–∞–µ–º —Å–∏–º–≤–æ–ª—ã –≤ —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
        total_chars = sum(len(msg.get("content", "")) for msg in messages)
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        return base_tokens + (total_chars // chars_per_token)

    def smart_truncate_chunk(self, chunk_text, target_length, min_block_size=50):
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç, —Å–æ—Ö—Ä–∞–Ω—è—è –Ω–∞—á–∞–ª–æ, –∫–æ–Ω–µ—Ü –∏ —Å–ª—É—á–∞–π–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã.
        
        Args:
            chunk_text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–µ–∑–∫–∏
            target_length: –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞
            min_block_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–ª–æ–∫–∞ –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è
        
        Returns:
            str: –û–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        blocks = re.split(r'\n\n|\. ', chunk_text)
        blocks = [b for b in blocks if len(b) >= min_block_size]
        
        if not blocks or sum(len(b) for b in blocks) <= target_length:
            return chunk_text[:target_length]
        
        # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü
        intro = blocks[0]
        outro = blocks[-1]
        
        # –°–∫–æ–ª—å–∫–æ –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–ª–æ—Å—å –Ω–∞ —Å–µ—Ä–µ–¥–∏–Ω—É
        remaining_length = target_length - len(intro) - len(outro) - 20  # 20 –¥–ª—è –º–∞—Ä–∫–µ—Ä–æ–≤ –æ–±—Ä–µ–∑–∫–∏
        
        if remaining_length <= 0:
            # –ï—Å–ª–∏ –º–µ—Å—Ç–∞ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞—á–∞–ª–æ
            return intro[:target_length-10] + "..."
        
        # –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –±–ª–æ–∫–∏ –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã
        middle_blocks = blocks[1:-1]
        selected_blocks = []
        current_length = 0
        
        # –°–ª—É—á–∞–π–Ω–æ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –±–ª–æ–∫–∏
        random.shuffle(middle_blocks)
        
        for block in middle_blocks:
            if current_length + len(block) <= remaining_length:
                selected_blocks.append(block)
                current_length += len(block)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
        result = intro + " ... " + " ".join(selected_blocks) + " ... " + outro
        return result
    
    def _handle_context_overflow(self, response, api_url, headers, payload):
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ—á–Ω—ã–π —Ä–∞—Å—á–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –æ–±—Ä–µ–∑–∫–∏ –∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é –≤—ã–±–æ—Ä–∫—É —Ç–µ–∫—Å—Ç–∞.
        """
        error_msg = response.text
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {error_msg[:200]}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ
        max_tokens = self._extract_context_limit(error_msg)
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –ª–∏–º–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {max_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        original_messages = payload["messages"]
        system_msg = None
        last_user_msg = None
        
        for msg in original_messages:
            if msg["role"] == "system":
                system_msg = msg
            elif msg["role"] == "user":
                last_user_msg = msg
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        if not last_user_msg:
            return "Error: No user message found in context."
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
        current_tokens = self._estimate_tokens(original_messages)
        logger.info(f"–û—Ü–µ–Ω–∫–∞ —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {current_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ–±—Ä–µ–∑–∫–∏
        required_reduction_ratio = max_tokens / current_tokens if current_tokens > 0 else 0.5
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ—Ç 0.1 –¥–æ 0.9
        required_reduction_ratio = max(0.1, min(0.9, required_reduction_ratio))
        logger.info(f"–†–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ–±—Ä–µ–∑–∫–∏: {required_reduction_ratio:.2f}")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        metadata_tokens = 50  # Tokens for message metadata
        available_tokens = int(max_tokens * required_reduction_ratio) - metadata_tokens
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
        new_messages = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–ª—é —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è (–µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å)
        system_tokens = 0
        if system_msg:
            # –ú–∞–∫—Å–∏–º—É–º 10% –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
            max_system_tokens = min(int(available_tokens * 0.1), 200)
            max_system_chars = max_system_tokens * 4  # –ü—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
            
            # –ö–æ–ø–∏—Ä—É–µ–º –∏ –æ–±—Ä–µ–∑–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            system_copy = dict(system_msg)
            if len(system_copy["content"]) > max_system_chars:
                system_copy["content"] = self.smart_truncate_chunk(system_copy["content"], max_system_chars)
            
            new_messages.append(system_copy)
            system_tokens = max_system_tokens
        
        # –û—Å—Ç–∞–≤—à–∏–µ—Å—è —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_tokens = available_tokens - system_tokens
        user_chars = user_tokens * 4  # –ü—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤?
        content = last_user_msg["content"]
        is_file_merge = "–§–∞–π–ª 1:" in content and "–§–∞–π–ª 2:" in content
        
        if is_file_merge:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–∏—è–Ω–∏—è —Ñ–∞–π–ª–æ–≤
            intro_end = content.find("–§–∞–π–ª 1:")
            file1_start = intro_end
            file2_start = content.find("–§–∞–π–ª 2:")
            
            # –í—ã–¥–µ–ª—è–µ–º —á–∞—Å—Ç–∏
            intro = content[:intro_end].strip()
            file1_content = content[file1_start:file2_start].strip()
            file2_content = content[file2_start:].strip()
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            intro_chars = min(len(intro), int(user_chars * 0.1))
            remaining_chars = user_chars - intro_chars
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏ (–ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            file1_len = len(file1_content)
            file2_len = len(file2_content)
            total_files_len = file1_len + file2_len
            
            if total_files_len > 0:
                file1_ratio = file1_len / total_files_len
            else:
                file1_ratio = 0.5
            
            file1_chars = min(file1_len, int(remaining_chars * file1_ratio))
            file2_chars = min(file2_len, int(remaining_chars * (1 - file1_ratio)))
            
            # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–µ–º —á–∞—Å—Ç–∏
            intro_trimmed = intro[:intro_chars]
            file1_trimmed = self.smart_truncate_chunk(file1_content, file1_chars)
            file2_trimmed = self.smart_truncate_chunk(file2_content, file2_chars)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–æ–≤—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            new_content = f"{intro_trimmed}\n\n–§–∞–π–ª 1:{file1_trimmed}\n\n–§–∞–π–ª 2:{file2_trimmed}\n\n[–í–Ω–∏–º–∞–Ω–∏–µ: —Ñ–∞–π–ª—ã –±—ã–ª–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–Ω—ã –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞]"
        else:
            # –û–±—ã—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ - –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞
            new_content = self.smart_truncate_chunk(content, user_chars)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
        user_copy = dict(last_user_msg)
        user_copy["content"] = new_content
        new_messages.append(user_copy)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º payload
        payload["messages"] = new_messages
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        estimated_tokens = self._estimate_tokens(new_messages)
        logger.info(f"–û—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤ (–ª–∏–º–∏—Ç: {max_tokens})")
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å —Å –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        try:
            logger.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ ({len(new_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π)")
            retry_response = requests.post(api_url, headers=headers, json=payload, timeout=600)
            
            if retry_response.status_code == 200:
                result = retry_response.json()
                content = result["choices"][0]["message"]["content"]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
                warning = f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–Ω—Ç–µ–Ω—Ç –±—ã–ª –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–Ω –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –º–æ–¥–µ–ª–∏. –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —á–∞—Å—Ç–∏ —Å–æ–æ–±—â–µ–Ω–∏—è.\n\n"
                return warning + content
            else:
                # –ï—Å–ª–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –Ω–µ –ø–æ–º–æ–≥–ª–∞, –¥–µ–ª–∞–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ
                logger.warning(f"–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å: {retry_response.status_code} - {retry_response.text[:100]}")
                
                # –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–µ –æ–±—Ä–µ–∑–∞–Ω–∏–µ
                if system_msg:
                    system_copy = {"role": "system", "content": "–ü–æ–º–æ–≥–∏ —Å –∞–Ω–∞–ª–∏–∑–æ–º."}
                    extreme_messages = [system_copy]
                else:
                    extreme_messages = []
                
                if is_file_merge:
                    user_content = "–û–±—ä–µ–¥–∏–Ω–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –¥–≤—É—Ö —Ñ–∞–π–ª–æ–≤ (–æ–Ω–∏ –±—ã–ª–∏ –æ–±—Ä–µ–∑–∞–Ω—ã –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)."
                else:
                    user_content = "–°–æ–æ–±—â–µ–Ω–∏–µ –±—ã–ª–æ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º. –ü–æ–º–æ–≥–∏ —Å –æ—Ç–≤–µ—Ç–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç—Ç–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: " + content[:200]
                
                user_copy = {"role": "user", "content": user_content}
                extreme_messages.append(user_copy)
                
                payload["messages"] = extreme_messages
                
                # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–Ω–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
                extreme_response = requests.post(api_url, headers=headers, json=payload, timeout=600)
                
                if extreme_response.status_code == 200:
                    result = extreme_response.json()
                    content = result["choices"][0]["message"]["content"]
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Å–∏–ª—å–Ω–æ–π –æ–±—Ä–µ–∑–∫–µ
                    warning = f"‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ö–æ–Ω—Ç–µ–Ω—Ç –±—ã–ª —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –æ–±—Ä–µ–∑–∞–Ω –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –º–æ–¥–µ–ª–∏. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è.\n\n"
                    return warning + content
                else:
                    return f"Error: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å –¥–∞–∂–µ –ø–æ—Å–ª–µ —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤. –ö–æ–¥ –æ—à–∏–±–∫–∏: {extreme_response.status_code}"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ–±—Ä–µ–∑–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}")
            return f"Error: –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"

    def query(self, messages: Union[List[Dict], str], 
              model: str = "local-model", 
              temperature: float = None,
              stream: bool = False,
              maintain_history: bool = True) -> str:
        """Send query to API."""
        api_url = f"{self.base_url}/chat/completions"
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if temperature is None:
            temperature = self.temperature
        
        # Convert string to message dict if needed
        if isinstance(messages, str):
            messages = [{"role": "user", "content": messages}]
        
        # Prepare messages for API
        if maintain_history:
            # Add only new user message to history
            if len(messages) > 0 and messages[-1]["role"] == "user":
                self.message_history.append(messages[-1])
            
            messages_to_send = self.message_history
        else:
            # Use only provided messages
            messages_to_send = messages
        
        # Prepare payload
        payload = {
            "model": model,
            "messages": messages_to_send,
            "temperature": temperature,
            "stream": stream
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        try:
            logger.info(f"Sending request to {api_url}")
            response = requests.post(api_url, headers=headers, json=payload, timeout=600)
            
            if response.status_code == 200:
                result = response.json()
                content = result["choices"][0]["message"]["content"]
                
                # Add response to history if maintaining it
                if maintain_history:
                    self.message_history.append({"role": "assistant", "content": content})
                    
                    # Trim history if needed
                    if len(self.message_history) > self.max_history_length + 1:  # +1 for system
                        system_msg = self.message_history[0]
                        self.message_history = [system_msg] + self.message_history[-(self.max_history_length):]
                
                return content
            elif response.status_code == 400 and "context" in response.text and "tokens" in response.text:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                content = self._handle_context_overflow(response, api_url, headers, payload)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if maintain_history and not content.startswith("Error:"):
                    # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –∏–∑ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏
                    content_without_warning = re.sub(r'^‚ö†Ô∏è WARNING:[^\n]*\n\n', '', content)
                    self.message_history.append({"role": "assistant", "content": content_without_warning})
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –Ω–∞ –æ—Å–Ω–æ–≤–µ payload, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –¥–ª—è —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                    self.message_history = payload["messages"] + [{"role": "assistant", "content": content_without_warning}]
                
                return content
            else:
                error_msg = f"API Error: {response.status_code} - {response.text[:200]}"
                logger.error(error_msg)
                return f"Error: Cannot connect to API ({error_msg})"
                
        except Exception as e:
            error_msg = f"Error: {str(e)[:200]}"
            logger.error(error_msg)
            return f"Error: Cannot connect to API ({error_msg})"
    
    def query_stream(self, messages: Union[List[Dict], str], 
                    model: str = "local-model",
                    temperature: float = None,
                    maintain_history: bool = True) -> Iterator[str]:
        """Stream responses from API."""
        api_url = f"{self.base_url}/chat/completions"
        
        if temperature is None:
            temperature = self.temperature
        
        if isinstance(messages, str):
            messages = [{"role": "user", "content": messages}]
        
        if maintain_history:
            if len(messages) > 0 and messages[-1]["role"] == "user":
                self.message_history.append(messages[-1])
            
            messages_to_send = self.message_history
        else:
            messages_to_send = messages
        
        payload = {
            "model": model,
            "messages": messages_to_send,
            "temperature": temperature,
            "stream": True
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        try:
            response = requests.post(api_url, headers=headers, json=payload, stream=True)
            
            if response.status_code == 200:
                full_content = ""
                
                for line in response.iter_lines():
                    if line:
                        line = line.decode('utf-8')
                        
                        if line.startswith('data: ') and line != 'data: [DONE]':
                            try:
                                data = json.loads(line[6:])
                                if 'choices' in data and data['choices']:
                                    content = data['choices'][0]['delta'].get('content', '')
                                    if content:
                                        full_content += content
                                        yield content
                            except json.JSONDecodeError:
                                logger.error(f"Failed to parse JSON: {line}")
            
            if maintain_history and full_content:
                self.message_history.append({"role": "assistant", "content": full_content})
                
                if len(self.message_history) > self.max_history_length + 1:
                    system_msg = self.message_history[0]
                    self.message_history = [system_msg] + self.message_history[-(self.max_history_length):]
            elif response.status_code == 400 and "context" in response.text and "tokens" in response.text:
            # –ü—Ä–∏ –æ—à–∏–±–∫–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
                yield "‚ö†Ô∏è WARNING: Context too large. Only kept system and latest query.\n\n"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
            match = re.search(r'context length of only (\d+) tokens', response.text)
            max_tokens = int(int(match.group(1)) * 0.9 if match else 4000)
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            new_messages = []
            system_msg = None
            last_user_msg = None
            
            for msg in messages_to_send:
                if msg["role"] == "system":
                    system_msg = msg
                elif msg["role"] == "user":
                    last_user_msg = msg
            
            if system_msg:
                new_messages.append(system_msg)
            if last_user_msg:
                new_messages.append(last_user_msg)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞
            total_chars = sum(len(msg["content"]) for msg in new_messages)
            if total_chars / 4 > max_tokens and new_messages:
                # –£–¥–∞–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                if len(new_messages) > 1:
                    new_messages = [new_messages[1]]  # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                
                # –ï—Å–ª–∏ –¥–∞–∂–µ —ç—Ç–æ –Ω–µ –ø–æ–º–æ–≥–∞–µ—Ç, –æ–±—Ä–µ–∑–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                if len(new_messages[0]["content"]) / 4 > max_tokens:
                    max_chars = int(max_tokens * 4 * 0.8)
                    new_messages[0]["content"] = new_messages[0]["content"][:max_chars]
            
            # –û–±–Ω–æ–≤–ª—è–µ–º payload –∏ –ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞
            payload["messages"] = new_messages
            payload["stream"] = True
            
            retry_response = requests.post(api_url, headers=headers, json=payload, stream=True)
            
            if retry_response.status_code == 200:
                full_content = ""
                
                for line in retry_response.iter_lines():
                    if line:
                        line = line.decode('utf-8')
                        
                        if line.startswith('data: ') and line != 'data: [DONE]':
                            try:
                                data = json.loads(line[6:])
                                if 'choices' in data and data['choices']:
                                    content = data['choices'][0]['delta'].get('content', '')
                                    if content:
                                        full_content += content
                                        yield content
                            except json.JSONDecodeError:
                                logger.error(f"Failed to parse JSON: {line}")
                
                if maintain_history and full_content:
                    self.message_history = new_messages + [{"role": "assistant", "content": full_content}]
            else:
                yield f"Error: Still failed after truncating context: {retry_response.status_code}"
            
        except Exception as e:
            yield f"Error: {str(e)[:200]}"
    
    def query_with_retry(self, messages: Union[List[Dict], str], 
                         model: str = "local-model",
                         temperature: float = None,
                         max_retries: int = 3, 
                         maintain_history: bool = True) -> str:
        """
        Try to query API with retries on failure.
        
        Args:
            messages: Either a string or list of message dicts
            model: Model identifier
            temperature: Temperature for generation (default from config or 0.2)
            max_retries: Maximum number of retries
            maintain_history: Whether to maintain conversation history
            
        Returns:
            str: Response from model
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if temperature is None:
            temperature = self.temperature
        
        for attempt in range(max_retries):
            try:
                response = self.query(
                    messages=messages,
                    model=model,
                    temperature=temperature,
                    stream=False,
                    maintain_history=maintain_history
                )
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ
                if "‚ö†Ô∏è WARNING: The context was too large" in response:
                    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ, –Ω–æ –Ω–µ –æ—à–∏–±–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    return response
                
                # Check if error
                if response.startswith("Error:"):
                    logger.warning(f"Attempt {attempt+1}/{max_retries} failed: {response}")
                    if attempt < max_retries - 1:
                        time.sleep(1)
                        continue
                
                return response
            except Exception as e:
                logger.error(f"Attempt {attempt+1}/{max_retries} failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)
        
        return "Error: Could not get response after multiple attempts"
    
    def clear_history(self):
        """Clear conversation history, keeping only the system message."""
        system_message = self.message_history[0]
        self.message_history = [system_message]
        logger.info("Conversation history cleared")
    
    def set_system_message(self, message: str):
        """
        Set a new system message.
        
        Args:
            message: New system message
        """
        if self.message_history and self.message_history[0]["role"] == "system":
            self.message_history[0]["content"] = message
        else:
            self.message_history.insert(0, {"role": "system", "content": message})
            
        logger.info(f"System message set to: {message[:50]}...")

    def list_available_models(self) -> List[str]:
        """
        List available models from the API.
        
        Returns:
            List[str]: List of model IDs
        """
        try:
            models_url = f"{self.base_url}/models"
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            response = requests.get(models_url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                models_data = response.json().get("data", [])
                model_ids = [model.get("id", "unknown") for model in models_data]
                return model_ids
            else:
                logger.warning(f"Failed to list models: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            logger.error(f"Error listing models: {str(e)}")
            return []

def save_dialog(dialog_history: List[Dict], output_dir: Optional[Path] = None):
    """
    Save dialog history to a file.
    
    Args:
        dialog_history: List of dialog entries
        output_dir: Optional output directory (default: "./dialogues")
    """
    if output_dir is None:
        output_dir = Path("./dialogues")
    
    output_dir.mkdir(exist_ok=True, parents=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{timestamp}_dialog.md"
    file_path = output_dir / filename
    
    content = f"# –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞\n"
    content += f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    for i, msg in enumerate(dialog_history):
        if msg["role"] == "system":
            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg["role"] == "user" else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
        content += f"## –°–æ–æ–±—â–µ–Ω–∏–µ {i}\n"
        content += f"**{role}:** {msg['content']}\n\n"
        content += "---\n\n"
    
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    logger.info(f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {file_path}")
    return file_path

def main():
    """Main function to run interactive CLI with OpenAI API."""
    
    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description="OpenAI API CLI")
    parser.add_argument("--url", default="http://localhost:1234/v1", help="API base URL")
    parser.add_argument("--key", default="lm-studio", help="API key")
    parser.add_argument("--system", default="You are an algorithm, that structurizes texts and finds new senses in documents", 
                       help="System message")
    parser.add_argument("--temperature", type=float, default=None, help="Temperature (default from config.yaml or 0.2)")
    parser.add_argument("--config", default="config.yaml", help="Path to config.yaml")
    args = parser.parse_args()
    
    # Create API instance
    api = OpenAI_API(
        base_url=args.url,
        api_key=args.key,
        system_message=args.system,
        config_path=args.config
    )
    
    # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –æ–Ω–∞ —É–∫–∞–∑–∞–Ω–∞
    if args.temperature is not None:
        api.temperature = args.temperature
    
    # Check connection
    if not api.check_connection():
        print("‚ö†Ô∏è Failed to connect to API. Make sure LM Studio is running.")
        retry = input("Retry connection? (y/n): ").lower()
        if retry != 'y':
            return
        
        if not api.check_connection():
            print("‚ö†Ô∏è Still can't connect. Exiting.")
            return
    
    # Get available models
    models = api.list_available_models()
    if models:
        print(f"Available models: {', '.join(models)}")
        model = models[0]  # Use first available model
    else:
        print("‚ö†Ô∏è No models found. Using default model name.")
        model = "local-model"
    
    print("\nü§ñ LM Studio Chat")
    print(f"System: \"{args.system}\"")
    print(f"Temperature: {api.temperature}")
    print("\nCommands:")
    print("  [save] - Save dialog history")
    print("  [clear] - Clear conversation history")
    print("  [exit] - Exit chat")
    print("  [merge] - Run insight generator")
    
    dialog_history = []
    
    while True:
        # Get user input
        user_input = input("\nYou: ").strip()
        
        # Handle commands
        if user_input.lower() == "[exit]":
            print("Goodbye! üëã")
            break
            
        elif user_input.lower() == "[clear]":
            api.clear_history()
            dialog_history = []
            print("Conversation history cleared.")
            continue
            
        elif user_input.lower() == "[save]":
            file_path = save_dialog(dialog_history)
            print(f"Dialog saved to {file_path}")
            continue
            
        elif user_input.lower() == "[merge]":
            print("\n--- –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∏–Ω—Å–∞–π—Ç–æ–≤ ---")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –¥–ª—è insight_finder
            components = {
                "vault_path": os.getcwd(),  # –¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
                "config_path": args.config,
                "api_client": api  # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–µ–∫—É—â–∏–π —ç–∫–∑–µ–º–ø–ª—è—Ä API
            }
            
            # –ü–µ—Ä–µ–¥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º run_insight_finder
            print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ—Ç–ø—Ä–∞–≤–∫–µ –∑–∞–ø—Ä–æ—Å–∞ –≤ LM-Studio...\n")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–Ω—Å–∞–π—Ç–æ–≤
            from insight_finder import run_insight_finder
            result = run_insight_finder(components)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            print("\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å–∞–π—Ç–æ–≤ ---")
            print(result)
            continue
        
        # Send query to API
        print("\nAssistant: ", end="", flush=True)
        
        # Use streaming for better user experience
        full_response = ""
        for chunk in api.query_stream(
            messages=user_input,
            model=model,
            temperature=api.temperature
        ):
            print(chunk, end="", flush=True)
            full_response += chunk
        
        print()  # Add newline after response
        
        # Add to dialog history
        dialog_history.append({
            "user": user_input,
            "assistant": full_response,
            "timestamp": datetime.now().isoformat()
        })

if __name__ == "__main__":
    main()